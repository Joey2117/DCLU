{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b06830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.platform import gfile\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import logging\n",
    "from __future__ import print_function, division, absolute_import, unicode_literals\n",
    "from typing import Union, Any\n",
    "import shutil\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "from keras import backend as K_1, regularizers\n",
    "from keras.engine.training import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, \\\n",
    "    BatchNormalization, Activation, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a60b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, images, labels, probs, indices, reshape=True):\n",
    "        \n",
    "        if reshape:\n",
    "            mean = np.mean(images,axis=(0,1,2,3))\n",
    "            std =  np.std(images,axis=(0,1,2,3))\n",
    "            images = (images - mean)/std\n",
    "        \n",
    "        self._images = images\n",
    "        self._num_examples = images.shape[0]\n",
    "        self._labels = labels\n",
    "        self._probs = probs\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        self._counter = np.zeros(self._num_examples)\n",
    "        self._indices = indices\n",
    "        self._subset_size = images.shape[0]\n",
    "        self._subset_ids = np.arange(self._num_examples)\n",
    "        self._data_limit = None\n",
    "        self._train_order = np.arange(self._num_examples)\n",
    "        self._epoch_ids = None\n",
    "        self._precent = 0.25\n",
    "        self._cur_precent = 0.25\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def probs(self):\n",
    "        return self._probs\n",
    "\n",
    "    @property\n",
    "    def indices(self):\n",
    "        return self._indices\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    @property\n",
    "    def counter(self):\n",
    "        return self._counter\n",
    "    \n",
    "    @property\n",
    "    def subset_size(self):\n",
    "        return self._subset_size\n",
    "\n",
    "    @property\n",
    "    def subset_ids(self):\n",
    "        return self._subset_ids\n",
    "    \n",
    "    def train_order(self):\n",
    "        pp = np.array(self._probs)\n",
    "        self._train_order = np.argsort(pp)\n",
    "        return self._train_order\n",
    "    \n",
    "    def revert_indicte(self):\n",
    "        self._indices = np.zeros(self._num_examples)\n",
    "    \n",
    "    def subset_step_indicate(self, epoch, increase_amount):\n",
    "        if epoch == 0:\n",
    "            self._data_limit = int(np.ceil(self._num_examples * self._precent))\n",
    "            self._epoch_ids = self._train_order[:self._data_limit]\n",
    "            self._indices[self._epoch_ids]  =  np.ones(self._data_limit)\n",
    "        else:\n",
    "            self._precent = min(self._cur_precent * increase_amount,1)\n",
    "            if self._precent != self._cur_precent:\n",
    "                self._cur_precent = self._precent\n",
    "                self._data_limit = int(np.ceil(self._num_examples * self._precent))\n",
    "                self._epoch_ids = self._train_order[:self._data_limit]\n",
    "                self._indices[self._epoch_ids]  =  np.ones(self._data_limit)\n",
    "            else:\n",
    "                self._data_limit = int(np.ceil(self._num_examples * self._precent))\n",
    "                self._epoch_ids = self._train_order[:self._data_limit]\n",
    "                self._indices[self._epoch_ids]  =  np.ones(self._data_limit)\n",
    "                \n",
    "                \n",
    "    def next_batch_train(self, batch_size):\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            self._epochs_completed += 1\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[self._train_order][start:end], \\\n",
    "                self._labels[self._train_order][start:end], \\\n",
    "                self._probs[self._train_order][start:end], \\\n",
    "                self._indices[self._train_order][start:end]    \n",
    "    \n",
    "    def next_batch_test(self, batch_size):    \n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "\n",
    "            # Shuffle the data\n",
    "            np.random.seed(1)\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            self._probs = self._probs[perm]\n",
    "            self._indices = self._indices[perm]\n",
    "\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[start:end], self._labels[start:end], self._probs[start:end], self._indices[start:end]\n",
    "\n",
    "    def change_probs(self, new_values):\n",
    "        self._probs[self._train_order] = new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afaaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_sets(datasetname, init_probs=[], one_hot=False):\n",
    "    \n",
    "    class DataSets(object):\n",
    "        pass\n",
    "    data_sets = DataSets()\n",
    "    \n",
    "    if datasetname == 'cifar10_keras':\n",
    "        (train_images,train_labels),(test_images,test_labels) = cifar10.load_data()\n",
    "    elif datasetname == 'mnist_keras':\n",
    "        (train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "        num_rows_train = train_images.shape[1]\n",
    "        num_clos_train = train_images.shape[2]\n",
    "        num_rows_test = test_images.shape[1]\n",
    "        num_clos_test = test_images.shape[2]\n",
    "        train_images = train_images.reshape(train_images.shape[0],num_rows_train,num_clos_train,1)\n",
    "        test_images = test_images.reshape(test_images.shape[0],num_rows_train,num_clos_train,1)\n",
    "        if K_1.image_data_format()=='channels_last':\n",
    "            train_images = train_images.transpose(0,1,2,3)\n",
    "            test_images = test_images.transpose(0,1,2,3)\n",
    "    else:\n",
    "        raise NotImplementedError('dataset not supported')\n",
    "    \n",
    "    if one_hot:\n",
    "        train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "        test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "  \n",
    "    n_test = test_images.shape[0]\n",
    "    n_train = train_images.shape[0]\n",
    "    \n",
    "    if not init_probs:\n",
    "        print('RANDOM INIT PROBABILITIES')\n",
    "        np.random.seed(3)\n",
    "        probs = np.random.rand(n_train)\n",
    "    else:\n",
    "        init_probs = np.asarray(init_probs)\n",
    "        probs_class = np.asarray(1.0 * init_probs / np.sum(init_probs), np.float32)\n",
    "        dense_train_labels = np.argmax(train_labels, axis=1)\n",
    "        probs = np.zeros_like(dense_train_labels, np.float32)\n",
    "        for k in range(0, np.unique(dense_train_labels).max()+1):\n",
    "            i = np.where(dense_train_labels == k)[0]\n",
    "            probs[i] = probs_class[k]\n",
    "\n",
    "    train_probs = np.squeeze(probs)\n",
    "    test_probs = np.squeeze(normalize(np.expand_dims(np.ones(n_test, np.float32), 1), axis=0, norm='l1'))\n",
    "    \n",
    "    train_indices = np.zeros(n_train)\n",
    "   \n",
    "    test_indices = np.zeros(n_test)\n",
    "    \n",
    "    data_sets.train = DataSet(train_images, train_labels, train_probs, train_indices)\n",
    "    \n",
    "    data_sets.test = DataSet(test_images, test_labels, test_probs, test_indices)\n",
    "    \n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afff66bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM INIT PROBABILITIES\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = read_data_sets('cifar10_keras', init_probs=[],one_hot=True)\n",
    "train_set = dataset_1.train\n",
    "test_set = dataset_1.test\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c2a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_evidence(logits):\n",
    "    return tf.nn.relu(logits)\n",
    "\n",
    "def exp_evidence(logits): \n",
    "    return tf.exp(tf.clip_by_value(logits/10,-10,10))\n",
    "\n",
    "def softplus_evidence(logits):\n",
    "    return tf.nn.softplus(logits)\n",
    "def softsign_evidence(logits):\n",
    "    return tf.nn.softsign(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9565e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(name, shape, init=None):\n",
    "    init = tf.truncated_normal_initializer(stddev=(1 / shape[0]) ** 0.5) if init is None else init\n",
    "    return tf.get_variable(name=name, shape=shape, dtype=tf.float32, initializer=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae035d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEAL_dense_layer(x, units, rate, activation, scope='DEAL_dense_0'):\n",
    "    with tf.variable_scope(scope):\n",
    "        W3 = var('W3', [x.get_shape()[1], 1000])\n",
    "        b3 = var('b3', [1000])\n",
    "        out3 = Activation(activation=activation)(tf.matmul(x, W3) + b3)\n",
    "        out3 = Dropout(rate=rate)(out3)\n",
    "\n",
    "        W4 = var('W4', [1000, units])\n",
    "        b4 = var('b4', [units])\n",
    "        logits = tf.matmul(out3, W4) + b4\n",
    "        return logits, W3, W4\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eac622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x, units=10,logits2evidence=softplus_evidence,activation='elu', dropout_1_rate=0.25, dropout_2_rate=0.5,\n",
    "            reg_factor=50e-4, bias_reg_factor=None, batch_norm=False):\n",
    "    \n",
    "    l2_reg = regularizers.l2(reg_factor)  # K.variable(K.cast_to_floatx(reg_factor))\n",
    "    l2_bias_reg = None\n",
    "    if bias_reg_factor:\n",
    "        l2_bias_reg = regularizers.l2(bias_reg_factor)  # K.variable(K.cast_to_floatx(bias_reg_factor))\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_regularizer=l2_reg,\n",
    "               bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_regularizer=l2_reg,\n",
    "                   bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(rate=dropout_1_rate)(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_regularizer=l2_reg,\n",
    "                bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', kernel_regularizer=l2_reg,\n",
    "                   bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(rate=dropout_1_rate)(x)\n",
    "\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_regularizer=l2_reg,\n",
    "                   bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3), padding='same', kernel_regularizer=l2_reg,\n",
    "                   bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(rate=dropout_1_rate)(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=(2, 2), padding='same', kernel_regularizer=l2_reg,\n",
    "                bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(2, 2), padding='same', kernel_regularizer=l2_reg,\n",
    "                   bias_regularizer=l2_bias_reg)(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(rate=dropout_1_rate)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x,W3,W4 = DEAL_dense_layer(x, units, rate=dropout_2_rate, activation=activation, scope='logit')\n",
    "#     x = Dense(units=128, kernel_regularizer=l2_reg, bias_regularizer=l2_bias_reg)(x)\n",
    "#     if batch_norm:\n",
    "#         x = BatchNormalization()(x)\n",
    "#     x = Activation(activation=activation)(x)\n",
    "\n",
    "#     x = Dropout(rate=dropout_2_rate)(x)\n",
    "#     x = Dense(units=units, kernel_regularizer=l2_reg, bias_regularizer=l2_bias_reg)(x)\n",
    "    evidence = logits2evidence(x)\n",
    "    return x, evidence, W3, W4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aec0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(alpha, K):\n",
    "    beta = tf.constant(np.ones((1, K)), dtype=tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "\n",
    "    KL = tf.reduce_sum((alpha - beta) * (tf.digamma(alpha) - tf.digamma(S_alpha)), axis=1, keepdims=True) + \\\n",
    "         tf.lgamma(S_alpha) - tf.reduce_sum(tf.lgamma(alpha), axis=1, keepdims=True) + \\\n",
    "         tf.reduce_sum(tf.lgamma(beta), axis=1, keepdims=True) - tf.lgamma(tf.reduce_sum(beta, axis=1, keepdims=True))\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8442ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_eq5(p, alpha, K, global_step, annealing_step):\n",
    "#     S = tf.reduce_sum(alpha, axis=1, keepdims=True)\n",
    "#     loglikelihood = tf.reduce_sum((p - (alpha / S)) ** 2, axis=1, keepdims=True) + tf.reduce_sum(\n",
    "#       alpha * (S - alpha) / (S * S * (S + 1)), axis=1, keepdims=True)\n",
    "#     KL_reg = tf.minimum(1.0, tf.cast(global_step / annealing_step, tf.float32)) * KL((alpha - 1) * (1 - p) + 1, K)\n",
    "#     return loglikelihood + KL_reg\n",
    "\n",
    "def loss_eq4(p, alpha, K, global_step, annealing_step, v):\n",
    "    loglikelihood = tf.reduce_mean(\n",
    "    tf.reduce_sum(p * (tf.digamma(tf.reduce_sum(alpha, axis=1, keepdims=True)) - tf.digamma(alpha)), 1,\n",
    "                    keepdims=True))\n",
    "    KL_reg = tf.minimum(1.0, tf.cast(global_step / annealing_step, tf.float32)) * KL((alpha - 1) * (1 - p) + 1, K)\n",
    "    loss = loglikelihood + KL_reg\n",
    "    final_loss = tf.multiply(loss, v)\n",
    "    return final_loss\n",
    "# def loss_eq3(p, alpha, K, global_step, annealing_step):\n",
    "#     loglikelihood = tf.reduce_mean(\n",
    "#     tf.reduce_sum(p * (tf.log(tf.reduce_sum(alpha, axis=1, keepdims=True)) - tf.log(alpha)), 1, keepdims=True))\n",
    "#     KL_reg = tf.minimum(1.0, tf.cast(global_step / annealing_step, tf.float32)) * KL((alpha - 1) * (1 - p) + 1, K)\n",
    "#     return loglikelihood + KL_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a17034",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 100 #batch size\n",
    "n_batches = np.ceil(1.0 * train_set.num_examples / bsize).astype(np.int32)\n",
    "n_batches_1 = np.ceil(1.0 * test_set.num_examples / bsize).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19f23614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_shd(epoch):\n",
    "#     if epoch >=0 and epoch <=20:\n",
    "#         lr = 0.001\n",
    "#     else:\n",
    "#         lr = 0.0001\n",
    "#     return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43ce4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_shd(epoch):\n",
    "    if epoch >=0 and epoch <=15:\n",
    "        lr = 0.001\n",
    "    else:\n",
    "        lr = 0.0001\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "039f8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resent_EDL(units=10,logits2evidence=softplus_evidence,activation='elu',reg_factor=50e-4, \n",
    "               bias_reg_factor=None, batch_norm=False,loss_function=loss_eq4,lmb=0.005):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        \n",
    "        X = tf.placeholder(shape=[None,32,32,3], dtype=tf.float32)\n",
    "        Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "        keep_prob_1 = tf.placeholder(dtype=tf.float32)\n",
    "        keep_prob_2 = tf.placeholder(dtype=tf.float32)\n",
    "        global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "        annealing_step = tf.placeholder(dtype=tf.int32) \n",
    "        v = tf.placeholder(dtype=tf.float32)\n",
    "        lr = tf.placeholder(dtype=tf.float32)\n",
    "        \n",
    "        logits, evidence,W3,W4 = network(X, units=10,logits2evidence=softplus_evidence,activation='elu', dropout_1_rate=keep_prob_1, \n",
    "                                          dropout_2_rate=keep_prob_2,reg_factor=50e-4, bias_reg_factor=None, batch_norm=False)   \n",
    "        alpha = evidence + 1        \n",
    "        u = K / tf.reduce_sum(alpha, axis=1, keepdims=True) #uncertainty\n",
    "        prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True) \n",
    "        \n",
    "        loss = tf.reduce_mean(loss_function(Y, alpha,K ,global_step, annealing_step, v))\n",
    "        l2_loss = (tf.nn.l2_loss(W3)+tf.nn.l2_loss(W4)) * lmb\n",
    "        total_loss = loss + l2_loss\n",
    "#         step  = tf.train.GradientDescentOptimizer(lr).minimize(total_loss, global_step=global_step)\n",
    "\n",
    "        step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        pred = tf.argmax(prob, 1)\n",
    "        truth = tf.argmax(Y, 1)\n",
    "        match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
    "        acc = tf.reduce_mean(match)\n",
    "\n",
    "        \n",
    "        return g, step, X, Y, v, annealing_step, prob, acc, loss, u, keep_prob_1, keep_prob_2,lr, evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d223b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2, step2, X2, Y2, v2, annealing_step, prob2, acc2, loss2,  u, keep_prob_1,keep_prob_2,lr,evidence = Resent_EDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "716e9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess2 = tf.Session(graph=g2)\n",
    "with g2.as_default():\n",
    "    sess2.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d5ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 - 100%) training:  acc: 0.2029 loss: 0.5103 \t testing: acc: 0.1791\n",
      "best epoch: 1 best acc: 0.1791\n",
      "epoch 2 - 100%) training:  acc: 0.2089 loss: 0.6378 \t testing: acc: 0.1820\n",
      "best epoch: 2 best acc: 0.1820\n",
      "epoch 3 - 100%) training:  acc: 0.3111 loss: 0.6996 \t testing: acc: 0.2302\n",
      "best epoch: 3 best acc: 0.2302\n",
      "epoch 4 - 100%) training:  acc: 0.3792 loss: 0.7308 \t testing: acc: 0.2281\n",
      "best epoch: 3 best acc: 0.2302\n",
      "epoch 5 - 100%) training:  acc: 0.4795 loss: 0.7798 \t testing: acc: 0.3500\n",
      "best epoch: 5 best acc: 0.3500\n",
      "epoch 6 - 100%) training:  acc: 0.5924 loss: 0.7800 \t testing: acc: 0.4562\n",
      "best epoch: 6 best acc: 0.4562\n",
      "epoch 7 - 100%) training:  acc: 0.6729 loss: 0.8738 \t testing: acc: 0.5450\n",
      "best epoch: 7 best acc: 0.5450\n",
      "epoch 8 - 100%) training:  acc: 0.7375 loss: 1.0224 \t testing: acc: 0.6350\n",
      "best epoch: 8 best acc: 0.6350\n",
      "epoch 9 - 100%) training:  acc: 0.7827 loss: 1.1085 \t testing: acc: 0.7549\n",
      "best epoch: 9 best acc: 0.7549\n",
      "epoch 10 - 100%) training:  acc: 0.8155 loss: 1.0009 \t testing: acc: 0.7636\n",
      "best epoch: 10 best acc: 0.7636\n",
      "epoch 11 - 100%) training:  acc: 0.8374 loss: 0.9129 \t testing: acc: 0.7822\n",
      "best epoch: 11 best acc: 0.7822\n",
      "epoch 12 - 100%) training:  acc: 0.8565 loss: 0.8362 \t testing: acc: 0.7768\n",
      "best epoch: 11 best acc: 0.7822\n",
      "epoch 13 - 100%) training:  acc: 0.8665 loss: 0.8017 \t testing: acc: 0.7764\n",
      "best epoch: 11 best acc: 0.7822\n",
      "epoch 14 - 100%) training:  acc: 0.8774 loss: 0.7589 \t testing: acc: 0.7771\n",
      "best epoch: 11 best acc: 0.7822\n",
      "epoch 15 - 100%) training:  acc: 0.8783 loss: 0.7692 \t testing: acc: 0.7818\n",
      "best epoch: 11 best acc: 0.7822\n",
      "epoch 16 - 100%) training:  acc: 0.8878 loss: 0.7170 \t testing: acc: 0.7818\n",
      "best epoch: 11 best acc: 0.7822\n",
      "epoch 17 - 100%) training:  acc: 0.9399 loss: 0.4519 \t testing: acc: 0.8059\n",
      "best epoch: 17 best acc: 0.8059\n",
      "epoch 18 - 100%) training:  acc: 0.9515 loss: 0.3580 \t testing: acc: 0.8101\n",
      "best epoch: 18 best acc: 0.8101\n",
      "epoch 19 - 100%) training:  acc: 0.9565 loss: 0.3101 \t testing: acc: 0.8091\n",
      "best epoch: 18 best acc: 0.8101\n",
      "epoch 20 - 100%) training:  acc: 0.9597 loss: 0.2767 \t testing: acc: 0.8098\n",
      "best epoch: 18 best acc: 0.8101\n",
      "epoch 21 - 100%) training:  acc: 0.9616 loss: 0.2536 \t testing: acc: 0.8115\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 22 - 100%) training:  acc: 0.9631 loss: 0.2387 \t testing: acc: 0.8090\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 23 - 100%) training:  acc: 0.9639 loss: 0.2278 \t testing: acc: 0.8078\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 24 - 100%) training:  acc: 0.9641 loss: 0.2166 \t testing: acc: 0.8073\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 25 - 100%) training:  acc: 0.9647 loss: 0.2090 \t testing: acc: 0.8044\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 26 - 100%) training:  acc: 0.9650 loss: 0.2107 \t testing: acc: 0.7987\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 27 - 100%) training:  acc: 0.9656 loss: 0.1994 \t testing: acc: 0.7963\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 28 - 100%) training:  acc: 0.9663 loss: 0.1945 \t testing: acc: 0.7885\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 29 - 100%) training:  acc: 0.9665 loss: 0.1911 \t testing: acc: 0.7834\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 30 - 100%) training:  acc: 0.9667 loss: 0.1884 \t testing: acc: 0.7779\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 31 - 100%) training:  acc: 0.9673 loss: 0.1811 \t testing: acc: 0.7663\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 32 - 100%) training:  acc: 0.9674 loss: 0.1838 \t testing: acc: 0.7553\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 33 - 100%) training:  acc: 0.9679 loss: 0.1804 \t testing: acc: 0.7596\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 34 - 100%) training:  acc: 0.9681 loss: 0.1771 \t testing: acc: 0.7529\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 35 - 100%) training:  acc: 0.9682 loss: 0.1759 \t testing: acc: 0.7437\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 36 - 100%) training:  acc: 0.9682 loss: 0.1746 \t testing: acc: 0.7439\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 37 - 100%) training:  acc: 0.9685 loss: 0.1703 \t testing: acc: 0.7242\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 38 - 100%) training:  acc: 0.9685 loss: 0.1638 \t testing: acc: 0.7400\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 39 - 100%) training:  acc: 0.9686 loss: 0.1676 \t testing: acc: 0.7303\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 40 - 100%) training:  acc: 0.9683 loss: 0.1640 \t testing: acc: 0.7381\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 41 - 100%) training:  acc: 0.9685 loss: 0.1679 \t testing: acc: 0.7292\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 42 - 100%) training:  acc: 0.9688 loss: 0.1598 \t testing: acc: 0.7326\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 43 - 100%) training:  acc: 0.9686 loss: 0.1642 \t testing: acc: 0.7381\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 44 - 100%) training:  acc: 0.9688 loss: 0.1561 \t testing: acc: 0.7507\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 45 - 100%) training:  acc: 0.9688 loss: 0.1557 \t testing: acc: 0.7214\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 46 - 100%) training:  acc: 0.9688 loss: 0.1578 \t testing: acc: 0.7282\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 47 - 100%) training:  acc: 0.9683 loss: 0.1593 \t testing: acc: 0.7181\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 48 - 100%) training:  acc: 0.9686 loss: 0.1594 \t testing: acc: 0.7245\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 49 - 100%) training:  acc: 0.9683 loss: 0.1609 \t testing: acc: 0.7390\n",
      "best epoch: 21 best acc: 0.8115\n",
      "epoch 50 - 100%) training:  acc: 0.9688 loss: 0.1566 \t testing: acc: 0.7427\n",
      "best epoch: 21 best acc: 0.8115\n"
     ]
    }
   ],
   "source": [
    "L_train_acc1=[]\n",
    "L_test_acc1=[]\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(50):  \n",
    "    loss_trains = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    uncertainty = np.zeros(train_set.num_examples)\n",
    "    learning_rate = lr_shd(epoch)\n",
    "    train_set.train_order()\n",
    "    train_set.subset_step_indicate(epoch,increase_amount=1.2)\n",
    "    \n",
    "\n",
    "    start = 0\n",
    "    end = bsize\n",
    "    for i in range(n_batches):\n",
    "        data, label, probs, indices = train_set.next_batch_train(bsize)\n",
    "        feed_dict={X2:data, Y2:label, v2:indices, keep_prob_1:0.25, keep_prob_2:0.5, lr:learning_rate, annealing_step:50*n_batches}\n",
    "        _, train_loss, unc, train_acc1 = sess2.run([step2,loss2,u,acc2],feed_dict)\n",
    "        loss_trains.append(train_loss)\n",
    "        acc_train.append(train_acc1)\n",
    "        print('epoch %d - %d%%) '% (epoch+1, (100*(i+1))//n_batches), end='\\r' if i<n_batches-1 else '')\n",
    "        unc = np.array(unc).reshape((-1,))\n",
    "        if end > train_set.num_examples:\n",
    "            end = train_set.num_examples\n",
    "            unc = unc[:(end-start)]\n",
    "            \n",
    "        uncertainty[start:end] = unc\n",
    "        start += data.shape[0]\n",
    "        end += data.shape[0]\n",
    "    train_set.change_probs(uncertainty)\n",
    "    train_set.revert_indicte()\n",
    "    \n",
    "    for m in range(n_batches_1):\n",
    "        data2, label2, probs2, indices2 = test_set.next_batch_test(bsize)\n",
    "        feed_dict={X2:data2, Y2:label2,keep_prob_1:1.,keep_prob_2:1.}\n",
    "        test_acc1 = sess2.run(acc2, feed_dict)\n",
    "        acc_test.append(test_acc1)\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    loss_train = np.mean(loss_trains)\n",
    "    train_acc = np.mean(acc_train)\n",
    "    test_acc = np.mean(acc_test)\n",
    "\n",
    "    \n",
    "    L_train_acc1.append(train_acc)\n",
    "    L_test_acc1.append(test_acc)\n",
    "\n",
    "    print('training:  acc: %2.4f loss: %2.4f \\t testing: acc: %2.4f' % \n",
    "          (train_acc, loss_train, test_acc))\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_epoch = epoch + 1\n",
    "    print('best epoch: %d best acc: %2.4f' % (best_epoch, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c73600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
